{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_UkY-kKCw26d"
      },
      "outputs": [],
      "source": [
        "#!pip install torch\n",
        "#!pip install transformers\n",
        "#!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "# initialize tokenizer and model from pretrained GPT2 model from Huggingface\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-large', pad_token_id=tokenizer.eos_token_id, torch_dtype=torch.float16)  #, attn_implementation=\"flash_attention_2\")\n",
        "out = model.to(device)"
      ],
      "metadata": {
        "id": "VzKOE-XnxFry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate text - autoregressive next word prediction\n",
        "### note that `encode` and `decode` here do not refer to the encoder-decoder architecture, but encoding text into numeric representation and decoding back from numbers into words (tokens)\n"
      ],
      "metadata": {
        "id": "HXgId9kq4w2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"the first president of the US was\"\n",
        "\n",
        "#sequence = \\\n",
        "#\"All the lonely people \\\n",
        "#Where do they all come from? \"\n",
        "\n",
        "# encoding sentence for model to process\n",
        "inputs = tokenizer.encode(sequence, return_tensors='pt').to(device)\n",
        "\n",
        "# generating text and decoding, try sampling...\n",
        "outputs = model.generate(inputs, max_length=40, do_sample=False, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# printing output\n",
        "print('\\n\\n'+text)"
      ],
      "metadata": {
        "id": "3Yc4aks_xSRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef356e1-8fd9-4671-e186-15c446156ae8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "the first president of the US was not born in the United States. He was born on April 4, 1805, in New York City, the son of John Quincy Adams, a lawyer, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZY8pDOGzsEZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oej9V7nssEc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### denife useful functions for loading and processing data"
      ],
      "metadata": {
        "id": "DRhoeOFP4jwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(tokenizer=tokenizer, file_path=file_path, block_size=block_size,)\n",
        "    return dataset\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=mlm,)\n",
        "    return data_collator"
      ],
      "metadata": {
        "id": "drtA78FNqIRb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### domain adaptation of the pretrained GPT2 model"
      ],
      "metadata": {
        "id": "IsZydgPH5RZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_file_path, model_name, output_dir, overwrite_output_dir, per_device_train_batch_size, num_train_epochs, save_steps):\n",
        "\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "  data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "  model.save_pretrained(output_dir)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "          logging_steps=50,\n",
        "          report_to=\"none\",\n",
        "      )\n",
        "\n",
        "  trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "  trainer.save_model()"
      ],
      "metadata": {
        "id": "8oL-KxSRq1Ya"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2'\n",
        "#train_file_path = \"/content/fine-tuning-data/reddit.multiple.subreddits.10K.dat\"\n",
        "train_file_path = \"/content/domain-adaptation-data/shakespeare-5K.dat\"\n",
        "output_dir = '/content/adapted-models'\n",
        "\n",
        "overwrite_output_dir = False\n",
        "per_device_train_batch_size = 8\n",
        "num_train_epochs = 3\n",
        "save_steps = 500"
      ],
      "metadata": {
        "id": "09hWmwKYrFGF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "train(\n",
        "    output_dir=output_dir,\n",
        "    model_name=model_name,\n",
        "    train_file_path=train_file_path,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ],
      "metadata": {
        "id": "RfiRItISrkQd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "b0aa7fa8-a1b5-4cf9-eb59-5b4d61a96501"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [123/123 00:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.990500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.610200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate text with the freshly domain-adapted model"
      ],
      "metadata": {
        "id": "7P3Tcr6T5d9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/adapted-models'\n",
        "# initialize tokenizer and model from tuned GPT2 model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path, pad_token_id=tokenizer.eos_token_id, torch_dtype=torch.float16)\n",
        "out = model.to(device)"
      ],
      "metadata": {
        "id": "m4QeqJJbtflp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "sequence = \"the first president of the US was\"\n",
        "\n",
        "# encoding sentence for model to process\n",
        "inputs = tokenizer.encode(sequence, return_tensors='pt').to(device)\n",
        "\n",
        "# generating text and decoding, try sampling...\n",
        "outputs = model.generate(inputs, max_length=50, do_sample=False, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "# printing output\n",
        "print('\\n\\n'+text)"
      ],
      "metadata": {
        "id": "Laqmf6dDxsWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61024ed-7879-4ea6-afdd-084189aee8a6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "the first president of the US was born in the year of our Lord's birth.\n",
            "\n",
            "CAMILLO:\n",
            "I am sorry, sir, that you have not been able to see him,\n",
            "but I think you must have seen him\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P-DjkemZyD_N"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}