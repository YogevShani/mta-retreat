{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6cb9maCIfNcS"
      },
      "outputs": [],
      "source": [
        "#!pip install torch\n",
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "DBv0K9vygBK3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "id": "JKT6oLtQfNcV"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, tokenizer, sentence):\n",
        "    # encode the sentence using the tokenizer and return the model predictions\n",
        "    inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "        predictions = outputs[0]\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_6K9dXq6fNcW"
      },
      "outputs": [],
      "source": [
        "# alternative: gpt2-xl, and some others\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2-large')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "id": "VCQr9dPBfNcX"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(model, tokenizer, prefix):\n",
        "    predictions = get_predictions(model, tokenizer, prefix)\n",
        "\n",
        "    # get the next token candidates\n",
        "    next_token_candidates_tensor = predictions[0, -1, :]\n",
        "    # get the token probabilities for all candidates\n",
        "    all_candidates_probabilities = torch.nn.functional.softmax(next_token_candidates_tensor, dim=-1)\n",
        "    # get the top k next token candidates\n",
        "    topk_candidates_indexes = torch.topk(next_token_candidates_tensor, 20).indices.tolist()\n",
        "    # filter the token probabilities for the top k candidates\n",
        "    topk_candidates_probabilities = all_candidates_probabilities[topk_candidates_indexes].tolist()\n",
        "    # decode the top k candidates back to words\n",
        "    topk_candidates_tokens = [tokenizer.decode([idx]).strip() for idx in topk_candidates_indexes]\n",
        "    # return the top k candidates and their probabilities\n",
        "    for t, p in list(zip(topk_candidates_tokens, topk_candidates_probabilities)):\n",
        "        print(f'token: {t.ljust(30)}\\t prob: {round(p, 3)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS8jNKMLfNcX",
        "outputId": "c5360a03-806d-4d56-86c0-e58eca811b2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token: Germany                       \t prob: 0.495\n",
            "token: Berlin                        \t prob: 0.174\n",
            "token: the                           \t prob: 0.049\n",
            "token: a                             \t prob: 0.022\n",
            "token: Munich                        \t prob: 0.022\n",
            "token: Hamburg                       \t prob: 0.016\n",
            "token: Frankfurt                     \t prob: 0.011\n",
            "token: it                            \t prob: 0.009\n",
            "token: there                         \t prob: 0.009\n",
            "token: Cologne                       \t prob: 0.008\n",
            "token: Europe                        \t prob: 0.007\n",
            "token: this                          \t prob: 0.006\n",
            "token: Vienna                        \t prob: 0.005\n",
            "token: one                           \t prob: 0.005\n",
            "token: D                             \t prob: 0.004\n",
            "token: their                         \t prob: 0.004\n",
            "token: that                          \t prob: 0.003\n",
            "token: Dresden                       \t prob: 0.003\n",
            "token: Bav                           \t prob: 0.003\n",
            "token: G                             \t prob: 0.003\n"
          ]
        }
      ],
      "source": [
        "prefix = \"I heard that Germany is really amazing! Have you ever been to\"\n",
        "predict_next_word(model, tokenizer, prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNLymqrBfNcX",
        "outputId": "ba6b4eaf-649f-4ef6-e2c0-161c20cd67dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token: ?                             \t prob: 0.528\n",
            "token: ?\"                            \t prob: 0.306\n",
            "token: ,                             \t prob: 0.032\n",
            "token: or                            \t prob: 0.021\n",
            "token: before                        \t prob: 0.018\n",
            "token: and                           \t prob: 0.012\n",
            "token: yet                           \t prob: 0.007\n",
            "token: ?!                            \t prob: 0.007\n",
            "token: ??                            \t prob: 0.005\n",
            "token: ?\",                           \t prob: 0.005\n",
            "token: ?!\"                           \t prob: 0.005\n",
            "token: ?                             \t prob: 0.005\n",
            "token: ?'                            \t prob: 0.003\n",
            "token: ?\".                           \t prob: 0.003\n",
            "token: in                            \t prob: 0.002\n",
            "token: !?\"                           \t prob: 0.002\n",
            "token: (                             \t prob: 0.001\n",
            "token: then                          \t prob: 0.001\n",
            "token: !?                            \t prob: 0.001\n",
            "token: .                             \t prob: 0.001\n"
          ]
        }
      ],
      "source": [
        "prefix = \"I heard that Germany is really amazing! Have you ever been to Berlin\"\n",
        "predict_next_word(model, tokenizer, prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqZPhY0lfNcX",
        "outputId": "222b1ba4-fefc-4e53-935e-bc125d10d382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token: black                         \t prob: 0.119\n",
            "token: white                         \t prob: 0.079\n",
            "token: dark                          \t prob: 0.061\n",
            "token: a                             \t prob: 0.06\n",
            "token: very                          \t prob: 0.032\n",
            "token: like                          \t prob: 0.031\n",
            "token: different                     \t prob: 0.028\n",
            "token: not                           \t prob: 0.024\n",
            "token: brown                         \t prob: 0.024\n",
            "token: darker                        \t prob: 0.02\n",
            "token: so                            \t prob: 0.019\n",
            "token: all                           \t prob: 0.015\n",
            "token: light                         \t prob: 0.015\n",
            "token: Caucasian                     \t prob: 0.014\n",
            "token: the                           \t prob: 0.013\n",
            "token: Asian                         \t prob: 0.012\n",
            "token: ...                           \t prob: 0.011\n",
            "token: blue                          \t prob: 0.01\n",
            "token: really                        \t prob: 0.008\n",
            "token: Hispanic                      \t prob: 0.007\n"
          ]
        }
      ],
      "source": [
        "# our language is full of biases... and so language models\n",
        "\n",
        "prefix = \"Someone just broke into my car... His skin color was\"\n",
        "#prefix = \"Someone just was very kind to me at the store... His skin color was\"\n",
        "predict_next_word(model, tokenizer, prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErtaiTqIfNcY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}